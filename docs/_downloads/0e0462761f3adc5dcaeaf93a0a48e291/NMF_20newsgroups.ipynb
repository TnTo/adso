{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Analyze the 20newsgroups dataset with NMF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import adso\n\nimport matplotlib.pyplot as plt\n\nimport nltk\n\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "set seed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adso.set_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = adso.data.load_20newsgroups(split=\"test\")\n\nprint(\"Number of documents: \", len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenize the dataset using a stemmer and a stopwords list, removing punctation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "adso.transform.nltk_download(\"stopwords\")\n\nsnowball = nltk.stem.snowball.SnowballStemmer(\"english\")\n\n\ndef stemmer(word):\n    ret = snowball.stem(word)\n    if ret.isalpha():\n        return ret\n    else:\n        return None\n\n\ntokenizer = adso.transform.Tokenizer(\n    stemmer=stemmer,\n    stopwords=nltk.corpus.stopwords.words(\"english\") + [None],\n)\n\ntokens = tokenizer.fit_transform(data)\n\nprint(\"First ten tokens of the first document:\")\nprint(tokens[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform the list of tokens in a list of numbers.\nWe will use the frequency and the TFIDF frequency (a correction\nfor the distribution among the documents).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "freq = adso.transform.FreqVectorizer(max_freq=0.75, max_size=10000)\n\ntfidf = adso.transform.TFIDFVectorizer(max_freq=0.75, max_size=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the vocabulary and share it between the vectorizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "freq.fit(tokens)\n\n# I will write an ad hoc function later\nvocab = freq.vocab\n\nprint(\"Number of words in vocabulary: \", len(vocab))\n\ntfidf.vocab = vocab\n\nprint(\"index of word 'god': \", vocab[\"god\"])\nprint(\"word at index 32: \", vocab[32])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the frequency matrices from tokens.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "freq_matrix = freq.transform(tokens)\ntfidf_matrix = tfidf.transform(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NMF1 using frequency matrix and ACLS algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NMF1 = adso.topicmodel.NMF(\n    n_topic=20, max_iter=100, tolerance=1e-3, lambdaH=0.001, lambdaW=0.001\n)\nW1, H1, iter1 = NMF1.fit_transform(freq_matrix)\nprint(\"NMF1 ended after\", iter1, \"iterations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NMF2 using frequency matrix and AHCLS algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NMF2 = adso.topicmodel.NMF(\n    n_topic=20,\n    max_iter=100,\n    tolerance=1e-3,\n    lambdaH=0.001,\n    lambdaW=0.001,\n    alphaH=0.01,\n    alphaW=0.01,\n    method=\"AHCLS\",\n)\nW2, H2, iter2 = NMF2.fit_transform(freq_matrix)\nprint(\"NMF2 ended after\", iter2, \"iterations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NMF3 using tfidf matrix and ALS algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NMF3 = adso.topicmodel.NMF(n_topic=20, max_iter=100, tolerance=1e-3, method=\"ALS\")\nW3, H3, iter3 = NMF3.fit_transform(tfidf_matrix)\nprint(\"NMF3 ended after\", iter3, \"iterations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the 10 most characteristic words for the first topic of each model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"10 most characteristic words for the first topic of NMF1\")\nprint(\n    list(\n        map(\n            lambda i: vocab[i],\n            np.argsort(np.squeeze(-H1[0, :].toarray()))[:10].tolist(),\n        )\n    )\n)\nprint(\"10 most characteristic words for the first topic of NMF2\")\nprint(\n    list(\n        map(\n            lambda i: vocab[i],\n            np.argsort(np.squeeze(-H2[0, :].toarray()))[:10].tolist(),\n        )\n    )\n)\nprint(\"10 most characteristic words for the first topic of NMF3\")\nprint(\n    list(\n        map(\n            lambda i: vocab[i],\n            np.argsort(np.squeeze(-H3[0, :].toarray()))[:10].tolist(),\n        )\n    )\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the confusion matrix (not diagonalized) for NMF1\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_topic = np.argmax(W1, axis=1)\n\nlistvectorizer = adso.transform.ListVectorizer()\nlabels = list(map(lambda l: [l], data.get_labels()))\n\nlabel_topic = np.squeeze(listvectorizer.fit_transform(labels))\n\nconfusion = np.zeros((20, 20))\nfor i in zip(label_topic, predicted_topic):\n    confusion[i] += 1\n\nfig, ax = plt.subplots()\nax.imshow(confusion)\nax.set_xticks(np.arange(20))\nax.set_yticks(np.arange(20))\nax.set_yticklabels(list(listvectorizer.vocab.stoi.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the confusion matrix (not diagonalized) for NMF2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_topic = np.argmax(W2, axis=1)\n\nlistvectorizer = adso.transform.ListVectorizer()\nlabels = list(map(lambda l: [l], data.get_labels()))\n\nlabel_topic = np.squeeze(listvectorizer.fit_transform(labels))\n\nconfusion = np.zeros((20, 20))\nfor i in zip(label_topic, predicted_topic):\n    confusion[i] += 1\n\nfig, ax = plt.subplots()\nax.imshow(confusion)\nax.set_xticks(np.arange(20))\nax.set_yticks(np.arange(20))\nax.set_yticklabels(list(listvectorizer.vocab.stoi.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the confusion matrix (not diagonalized) for NMF3\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_topic = np.argmax(W3, axis=1)\n\nlistvectorizer = adso.transform.ListVectorizer()\nlabels = list(map(lambda l: [l], data.get_labels()))\n\nlabel_topic = np.squeeze(listvectorizer.fit_transform(labels))\n\nconfusion = np.zeros((20, 20))\nfor i in zip(label_topic, predicted_topic):\n    confusion[i] += 1\n\nfig, ax = plt.subplots()\nax.imshow(confusion)\nax.set_xticks(np.arange(20))\nax.set_yticks(np.arange(20))\nax.set_yticklabels(list(listvectorizer.vocab.stoi.keys()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}