"""Test load_txt function from data module."""

from adso.data.test import load_test_dataset
from adso.transform import Tokenizer, nltk_download

import nltk

nltk.stem.wordnet.WordNetLemmatizer().lemmatize


def test_tokenizer():
    data = load_test_dataset()

    simple_tokenizer = Tokenizer()
    assert simple_tokenizer.fit(data) is None
    assert (
        simple_tokenizer.fit_transform(data)
        == simple_tokenizer.transform(data)
        == [
            [
                "linear",
                "algebra",
                "studies",
                "matrices",
                ",",
                "vectors",
                "and",
                "vectorial",
                "spaces",
                ".",
                "sometimes",
                "linear",
                "algebra",
                "is",
                "considered",
                "a",
                "subfield",
                "of",
                "geometry",
                ".",
                "many",
                "theorem",
                "regarding",
                "matrices",
                "exists",
                "but",
                "one",
                "of",
                "the",
                "most",
                "important",
                "is",
                "the",
                "spectral",
                "one",
                ".",
            ],
            [
                "geometry",
                "studies",
                "shapes",
                "and",
                "entities",
                "in",
                "the",
                "space",
                ".",
                "a",
                "geometrician",
                "proves",
                "theorem",
                "about",
                "the",
                "relation",
                "among",
                "two",
                "or",
                "more",
                "geometrical",
                "entities",
                ".",
                "continuity",
                "is",
                "a",
                "geometric",
                "concept",
                "widely",
                "used",
                "in",
                "calculus",
                ".",
            ],
            [
                "dinosaurs",
                "are",
                "reptiles",
                ".",
                "birds",
                "descend",
                "from",
                "dinosaurs",
                ".",
                "even",
                "if",
                "most",
                "of",
                "the",
                "dinosaurs",
                "do",
                "n't",
                "fly",
                ",",
                "probably",
                "the",
                "majority",
                "of",
                "them",
                "was",
                "covered",
                "by",
                "feathers",
                ".",
            ],
            [
                "birds",
                "lay",
                "eggs",
                ",",
                "like",
                "reptiles",
                "and",
                "fishes",
                ".",
                "most",
                "of",
                "the",
                "birds",
                "fly",
                ",",
                "even",
                "if",
                "penguins",
                ",",
                "ostriches",
                "and",
                "some",
                "others",
                "are",
                "unable",
                "to",
                "fly",
                ".",
                "birds",
                "have",
                "two",
                "wings",
                ",",
                "like",
                "some",
                "dinosaurs",
                "and",
                "others",
                "ancient",
                "reptiles",
                ".",
            ],
        ]
    )

    stemming_tokenizer = Tokenizer(
        stemmer=nltk.stem.snowball.SnowballStemmer("english").stem
    )
    assert stemming_tokenizer.transform(data) == [
        [
            "linear",
            "algebra",
            "studi",
            "matric",
            ",",
            "vector",
            "and",
            "vectori",
            "space",
            ".",
            "sometim",
            "linear",
            "algebra",
            "is",
            "consid",
            "a",
            "subfield",
            "of",
            "geometri",
            ".",
            "mani",
            "theorem",
            "regard",
            "matric",
            "exist",
            "but",
            "one",
            "of",
            "the",
            "most",
            "import",
            "is",
            "the",
            "spectral",
            "one",
            ".",
        ],
        [
            "geometri",
            "studi",
            "shape",
            "and",
            "entiti",
            "in",
            "the",
            "space",
            ".",
            "a",
            "geometrician",
            "prove",
            "theorem",
            "about",
            "the",
            "relat",
            "among",
            "two",
            "or",
            "more",
            "geometr",
            "entiti",
            ".",
            "continu",
            "is",
            "a",
            "geometr",
            "concept",
            "wide",
            "use",
            "in",
            "calculus",
            ".",
        ],
        [
            "dinosaur",
            "are",
            "reptil",
            ".",
            "bird",
            "descend",
            "from",
            "dinosaur",
            ".",
            "even",
            "if",
            "most",
            "of",
            "the",
            "dinosaur",
            "do",
            "n't",
            "fli",
            ",",
            "probabl",
            "the",
            "major",
            "of",
            "them",
            "was",
            "cover",
            "by",
            "feather",
            ".",
        ],
        [
            "bird",
            "lay",
            "egg",
            ",",
            "like",
            "reptil",
            "and",
            "fish",
            ".",
            "most",
            "of",
            "the",
            "bird",
            "fli",
            ",",
            "even",
            "if",
            "penguin",
            ",",
            "ostrich",
            "and",
            "some",
            "other",
            "are",
            "unabl",
            "to",
            "fli",
            ".",
            "bird",
            "have",
            "two",
            "wing",
            ",",
            "like",
            "some",
            "dinosaur",
            "and",
            "other",
            "ancient",
            "reptil",
            ".",
        ],
    ]
